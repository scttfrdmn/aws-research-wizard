# Data Source Configurations for AWS Research Wizard
# Supports both static data repositories and real-time instrument feeds

# Static Data Sources
institutional_storage:
  type: "institutional_storage"
  description: "University HPC storage system with research datasets"
  connection:
    protocol: "sftp"
    hostname: "hpc-storage.university.edu"
    username: "research_user"
    key_file: "/home/user/.ssh/hpc_key"
    base_path: "/gpfs/research/datasets"

  research_domain: "multi_domain"
  data_categories:
    - "genomics"
    - "climate_modeling"
    - "geospatial_research"

  transfer_optimization:
    # Free ingress to AWS - optimize for throughput
    preferred_method: "parallel_multipart"
    max_concurrent_streams: 8
    chunk_size_mb: 100
    compression_enabled: true
    bandwidth_limit_mbps: null  # Use full available bandwidth

  cost_considerations:
    ingress_to_aws: 0.0  # Always free
    egress_waiver_eligible: true  # University qualifies for Global Data Egress Waiver
    storage_optimization: "intelligent_tiering"
    transfer_window: "off_peak_hours"  # 2 AM - 6 AM local time

# Research Consortium Shared Storage
consortium_data:
  type: "other_cloud"
  description: "Multi-institutional research data consortium"
  connection:
    provider: "azure_blob"
    account_name: "researchconsortium"
    container: "shared-datasets"
    access_key: "${AZURE_STORAGE_KEY}"

  research_domain: "climate_modeling"
  data_categories:
    - "atmospheric_chemistry"
    - "oceanography"

  transfer_optimization:
    # Cross-cloud transfer - coordinate timing carefully
    preferred_method: "staged_transfer"
    staging_location: "s3://research-staging-bucket/"
    verification_required: true
    checksum_validation: "sha256"

  cost_considerations:
    cross_cloud_egress: 0.087  # Azure to AWS per GB
    aws_ingress: 0.0
    coordination_overhead: "high"  # Requires careful scheduling

# Real-time Instrument Sources
genome_sequencer:
  type: "real_time_instrument"
  description: "Illumina NovaSeq 6000 sequencer"
  instrument_config:
    instrument_id: "novaseq_001"
    instrument_type: "file_watcher"
    data_rate_mb_per_hour: 1500  # ~1.5 GB/hour during sequencing
    data_format: "FASTQ"
    buffer_size_mb: 500

    # Monitor sequencer output directory
    watch_directory: "/mnt/sequencer/output"
    file_patterns: ["*.fastq.gz", "*.bam"]

    quality_control_rules:
      - type: "min_size"
        value: 50_000_000  # 50 MB minimum file size
      - type: "format_check"
        format: "fastq"
      - type: "read_count_check"
        min_reads: 1000000

    preprocessing_steps:
      - "fastqc_quality_check"
      - "adapter_trimming"
      - "duplicate_removal"

    trigger_conditions:
      data_availability: "file_complete"  # Wait for complete files
      min_trigger_size_gb: 2.0  # Don't trigger compute for small files
      max_buffer_time_hours: 4  # Process at least every 4 hours

  research_domain: "genomics"

  streaming_config:
    immediate_staging: true
    staging_location: "s3://genomics-staging/novaseq/"
    real_time_processing: true
    batch_aggregation: false

  cost_considerations:
    streaming_infrastructure: 0.05  # Per hour for real-time streaming
    staging_storage: 0.023  # S3 standard per GB/month
    compute_coordination: "trigger_on_availability"

# Environmental Sensor Network
environmental_sensors:
  type: "real_time_instrument"
  description: "Distributed environmental sensor network"
  instrument_config:
    instrument_id: "env_network_001"
    instrument_type: "http_stream"
    data_rate_mb_per_hour: 50  # Lower volume, continuous stream
    data_format: "JSON"
    buffer_size_mb: 100

    # HTTP streaming endpoint
    stream_endpoint: "https://sensors.research.edu/api/stream"
    authentication:
      type: "api_key"
      key: "${SENSOR_API_KEY}"

    quality_control_rules:
      - type: "timestamp_validation"
        max_delay_minutes: 15
      - type: "sensor_range_check"
        temperature_range: [-40, 60]  # Celsius
        humidity_range: [0, 100]  # Percent
      - type: "data_completeness"
        required_fields: ["timestamp", "sensor_id", "temperature", "humidity", "pressure"]

    preprocessing_steps:
      - "timestamp_standardization"
      - "unit_conversion"
      - "anomaly_detection"
      - "spatial_interpolation"

    trigger_conditions:
      data_availability: "time_based"
      trigger_frequency: "hourly"  # Process hourly aggregations
      batch_size_records: 1000
      alert_thresholds:
        extreme_temperature: 50  # Trigger immediate processing for extreme values
        sensor_failure: true  # Trigger diagnostics for missing data

  research_domain: "atmospheric_chemistry"

  streaming_config:
    immediate_staging: false  # Batch processing is sufficient
    staging_location: "s3://environmental-data/sensors/"
    real_time_processing: false
    batch_aggregation: true
    aggregation_window_hours: 1

  cost_considerations:
    streaming_infrastructure: 0.02  # Lower cost for batch processing
    data_compression: 0.7  # JSON compresses well
    processing_efficiency: "batch_optimized"

# Satellite Data Feed
satellite_imagery:
  type: "real_time_instrument"
  description: "Sentinel-2 satellite imagery feed"
  instrument_config:
    instrument_id: "sentinel2_feed"
    instrument_type: "http_api"
    data_rate_mb_per_hour: 2000  # High volume during satellite passes
    data_format: "COG"  # Cloud-Optimized GeoTIFF
    buffer_size_mb: 1000

    # API endpoint for new imagery notifications
    api_endpoint: "https://copernicus.eu/api/sentinel2/notifications"
    polling_interval_minutes: 30

    quality_control_rules:
      - type: "cloud_coverage"
        max_cloud_percent: 20
      - type: "geographic_bounds"
        min_lat: 30.0
        max_lat: 50.0
        min_lon: -130.0
        max_lon: -60.0  # Focus on North America
      - type: "image_quality"
        min_pixel_quality: 0.8

    preprocessing_steps:
      - "atmospheric_correction"
      - "cloud_masking"
      - "geometric_correction"
      - "tile_generation"

    trigger_conditions:
      data_availability: "scene_complete"
      min_trigger_area_km2: 1000  # Minimum scene size
      processing_priority: "geographic_relevance"
      max_processing_delay_hours: 24

  research_domain: "geospatial_research"

  streaming_config:
    immediate_staging: true  # Large files benefit from immediate staging
    staging_location: "s3://satellite-imagery/sentinel2/"
    real_time_processing: true
    batch_aggregation: false

  cost_considerations:
    high_volume_periods: "satellite_pass_windows"
    storage_optimization: "immediate_transition_to_ia"  # Infrequent Access after processing
    processing_coordination: "scene_based_triggers"

# Global Configuration
global_settings:
  # AWS Global Data Egress Waiver Configuration
  egress_waiver:
    enabled: true
    program: "AWS Global Data Egress Waiver"
    qualifying_institution: true
    waiver_limit_tb_monthly: 100  # 100 TB/month waiver limit
    monitoring_required: true
    usage_reporting: "monthly"

  # Transfer Optimization
  transfer_defaults:
    ingress_optimization: "maximize_throughput"  # Ingress is free
    staging_strategy: "regional_optimization"  # Use closest AWS region
    compression_threshold_gb: 1.0  # Compress files larger than 1 GB
    parallel_streams: 4  # Default parallel connections
    retry_attempts: 3
    timeout_hours: 24

  # Coordination Settings
  compute_coordination:
    wait_for_data_availability: true
    pre_warm_compute: false  # Don't spin up compute until data ready
    staging_verification: true  # Verify data integrity before compute
    cost_threshold_check: true  # Check costs before large operations

    # Intelligent scheduling windows
    preferred_compute_hours:
      - "02:00-06:00"  # Off-peak hours for large jobs
      - "22:00-24:00"  # Evening hours for medium jobs

    priority_overrides:
      real_time_instruments: "immediate"
      research_deadlines: "priority_queue"
      cost_sensitive: "off_peak_only"

  # Monitoring and Alerting
  monitoring:
    data_transfer_tracking: true
    cost_monitoring: true
    performance_metrics: true
    egress_waiver_usage: true

    alerts:
      approaching_egress_limit: 80  # Alert at 80% of waiver limit
      transfer_failures: true
      compute_idle_time: 30  # Alert if compute idle >30 minutes
      cost_threshold_exceeded: true

  # Integration Settings
  integration:
    workflow_engine: "demo_workflow_engine"
    cost_management: "finops_integration"
    scheduling: "intelligent_coordination"
    monitoring: "cloudwatch_integration"
