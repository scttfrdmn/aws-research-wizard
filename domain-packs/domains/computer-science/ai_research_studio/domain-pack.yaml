aws_config:
  instance_types:
    gpu: g5.xlarge
    hpc: c6i.4xlarge
    large: m6i.2xlarge
    medium: m6i.xlarge
    memory: r6i.2xlarge
    small: m6i.large
  network:
    efa_enabled: true
    enhanced_networking: true
    placement_group: true
  storage:
    iops: 3000
    size_gb: 500
    throughput: 125
    type: gp3
categories:
- Machine Learning
- Artificial Intelligence
- Computer Vision
- Natural Language Processing
cost_estimates:
  cpu_development: $5-20/day (prototyping, data prep)
  distributed: $500-2000/day (cluster training 8+ nodes)
  idle: $0/day (frameworks cached, no GPU usage)
  inference: $0.02-1.00 per 1000 predictions
  monthly_estimate: $300-2500/month for ML research team
  multi_gpu: $120-500/day (large model training)
  single_gpu: $30-120/day (model training V100/A100)
description: GPU-optimized machine learning and AI development environment
documentation:
  best_practices: docs/ai/ml_research_studio_best_practices.md
  getting_started: docs/ai/ml_research_studio_quickstart.md
  tutorials: docs/ai/ml_research_studio_tutorials.md
maintainers:
- email: aws-research-wizard@example.com
  name: AWS Research Wizard
  organization: AWS Research Computing
name: AI/ML Research Studio
spack_config:
  compiler: gcc@11.4.0
  optimization: -O3
  packages:
  - pytorch@2.0.1 %gcc@11.4.0 +cuda+nccl+magma+fbgemm
  - tensorflow@2.13.0 %gcc@11.4.0 +cuda+nccl+mkl
  - jax@0.4.13 %gcc@11.4.0 +cuda
  - onnx@1.14.0 %gcc@11.4.0
  - xgboost@1.7.6 %gcc@11.4.0 +cuda+nccl
  - lightgbm@4.0.0 %gcc@11.4.0 +cuda
  - opencv@4.8.0 %gcc@11.4.0 +python3+cuda+dnn+contrib
  - py-torchvision@0.15.2
  - py-albumentations@1.3.1
  - py-scikit-image@0.21.0
  - py-pillow@10.0.0
  - cuda@12.2.0
  - cudnn@8.9.2.26
  - nccl@2.18.3 +cuda
  - cutensor@1.7.0.1
  - cupy@12.2.0
  - magma@2.7.2 +cuda+fortran
  - ray@2.6.1 %gcc@11.4.0 +cuda
  - py-horovod@0.28.1 +cuda+nccl+pytorch+tensorflow
  - py-deepspeed@0.10.0
  - py-fairscale@0.4.13
  - python@3.11.4 %gcc@11.4.0 +optimizations+shared
  - py-numpy@1.25.1 ^openblas@0.3.23 threads=openmp
  - py-scipy@1.11.1 ^openblas@0.3.23
  - py-pandas@2.0.3
  - py-matplotlib@3.7.2
  - py-seaborn@0.12.2
  - py-plotly@5.15.0
  - py-jupyter@1.0.0
  - py-jupyterlab@4.0.3
  - py-mlflow@2.5.0
  - py-wandb@0.15.8
  - py-tensorboard@2.13.0
  - py-optuna@3.2.0
  - py-hydra-core@1.3.2
  - py-dvc@3.12.0
  target: neoverse_v1
version: 1.0.0
workflows:
- description: Sample Large language model fine-tuning (BERT, GPT, T5) workflow
  expected_output: Processed research data
  input_data: s3://aws-open-data/
  name: Large language model fine-tuning (BERT, GPT, T5)
  script: workflows/large_language_model_fine-tuning_(bert,_gpt,_t5).sh
- description: Sample Computer vision model training (ResNet, YOLO, ViT) workflow
  expected_output: Processed research data
  input_data: s3://aws-open-data/
  name: Computer vision model training (ResNet, YOLO, ViT)
  script: workflows/computer_vision_model_training_(resnet,_yolo,_vit).sh
- description: Sample Distributed training with multiple GPUs/nodes workflow
  expected_output: Processed research data
  input_data: s3://aws-open-data/
  name: Distributed training with multiple GPUs/nodes
  script: workflows/distributed_training_with_multiple_gpus/nodes.sh
